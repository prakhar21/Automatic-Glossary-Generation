{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named spacy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2fcead074a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mguten_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_g\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named spacy"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "words_r = reuters.words()\n",
    "words_b = brown.words()\n",
    "words_w = webtext.words()\n",
    "words_g = gutenberg.words()\n",
    "\n",
    "brown_words = dict(collections.Counter([lemmatizer.lemmatize(i.lower()) for i in words_b]))\n",
    "reuters_words = dict(collections.Counter([lemmatizer.lemmatize(i.lower()) for i in words_r]))\n",
    "web_words = dict(collections.Counter([lemmatizer.lemmatize(i.lower()) for i in words_w]))\n",
    "guten_words = dict(collections.Counter([lemmatizer.lemmatize(i.lower()) for i in words_g]))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_brown = [i[0] for i in sorted(brown_words.items(), key=lambda k: k[1], reverse=True) if i[1] > 100]\n",
    "stop_reuters = [i[0] for i in sorted(reuters_words.items(), key=lambda k: k[1], reverse=True) if i[1] > 250]\n",
    "stop_web = [i[0] for i in sorted(web_words.items(), key=lambda k: k[1], reverse=True) if i[1] > 50]\n",
    "stop_guten = [i[0] for i in sorted(guten_words.items(), key=lambda k: k[1], reverse=True) if i[1] > 100]\n",
    "\n",
    "len(stop_brown), len(stop_reuters), len(stop_web), len(stop_guten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'data'\n",
    "DATA_DIR = codecs.open(os.path.join(BASE_DIR, 'books/HP1.txt'), 'rb', encoding='utf-8').readlines()\n",
    "COMMON_DIR = codecs.open(os.path.join(BASE_DIR, 'google_10000.txt'), 'rb', encoding='utf-8').readlines()\n",
    "true_data = pd.read_csv(os.path.join(BASE_DIR, 'ground_truth/HP1.csv'), sep='\\t')\n",
    "stop_words = codecs.open(os.path.join(BASE_DIR, 'stop.txt'), 'rb', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.update(list(set([i.strip().lower() for i in stop_words])))\n",
    "stop.update(list(set([i.strip().lower() for i in stop_brown])))\n",
    "stop.update(list(set([i.strip().lower() for i in stop_web])))\n",
    "stop.update(list(set([i.strip().lower() for i in stop_reuters])))\n",
    "stop.update(list(set([i.strip().lower() for i in stop_guten])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data_lst = true_data.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "punctuation = \"'!()-[]{};:'\\,<>./?@#$%^&*_~\"\n",
    "\n",
    "def expand_contractions(data_str):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\", \"’\"]\n",
    "    for s in specials:\n",
    "        data_str = data_str.replace(s, \"'\")\n",
    "    data_str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in data_str.split(\" \")])\n",
    "    return data_str\n",
    "\n",
    "def extra_spaces(data_str):\n",
    "    return re.sub(r'\\s{1,}', ' ', data_str)\n",
    "\n",
    "def punctuations(data_str):\n",
    "    data_str = data_str.replace(\"'s\", \"\")\n",
    "    for x in data_str.lower():\n",
    "        if x in punctuation: \n",
    "            data_str = data_str.replace(x, \"\")\n",
    "    return data_str\n",
    "\n",
    "def remove_stop(data_str):\n",
    "    word_tokens = nltk.word_tokenize(data_str)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def preprocess(data_str):\n",
    "    data_str = expand_contractions(data_str)\n",
    "    data_str = remove_stop(data_str)\n",
    "    data_str = punctuations(data_str)\n",
    "    data_str = extra_spaces(data_str)\n",
    "    return data_str\n",
    "\n",
    "def precision(pred, actual):\n",
    "    N = [i for i in pred if i in actual]\n",
    "    _D = [i for i in pred if i not in actual]\n",
    "    return len(N) / (len(N)+len(_D)), N, _D\n",
    "\n",
    "def recall(pred, actual):\n",
    "    N = [i for i in pred if i in actual]\n",
    "    _D = [i for i in actual if i not in pred]\n",
    "    return len(N) / (len(N)+len(_D)), N, _D\n",
    "\n",
    "def f1score(precision, recall):\n",
    "    return (2*precision*recall) / (precision + recall)\n",
    "\n",
    "def ner(data):\n",
    "    ner_chunks = set()\n",
    "    ner_chunks_cat = set()\n",
    "    doc = nlp(data)\n",
    "    for ent in doc.ents:\n",
    "        ner_chunks.add(ent.text)\n",
    "        ner_chunks_cat.add(ent.label_)\n",
    "    return ner_chunks, ner_chunks_cat\n",
    "\n",
    "full_data = ' '.join([i.strip() for i in DATA_DIR if len(i.strip())>1])\n",
    "# ner_chunks, ner_chunks_cat = ner(full_data)\n",
    "# for i in ner_chunks:\n",
    "#     full_data = full_data.replace(i, '')\n",
    "full_data_ = full_data.lower()\n",
    "data = nltk.sent_tokenize(full_data_)\n",
    "#data = [i for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = set([i for i in true_data_lst if i in ' '.join(data).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.summarization import keywords\n",
    "# np_chunkss = keywords(' '.join(data), lemmatize=True).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np_chunkss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# # np_chunks = set()\n",
    "# # doc = nlp(data)\n",
    "# # for chunk in doc.noun_chunks:\n",
    "# #     np_chunks.add(chunk.text)\n",
    "\n",
    "# ######\n",
    "# #np_chunks = set(nltk.word_tokenize(data))\n",
    "\n",
    "# ######\n",
    "np_chunks = []\n",
    "is_noun = lambda pos: pos[:2] in ['NN', 'JJ']\n",
    "for sent in data:\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    np_chunks.append([word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunks = set([lemmatizer.lemmatize(item.strip(punctuation)) for sublist in np_chunks for item in sublist])\n",
    "np_chunks = [i for i in np_chunks if '-' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_chunks = set([lemmatizer.lemmatize(item.strip(punctuation)) for item in ['sense']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_chunks.extend(np_chunkss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunks = set(np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.020177562550443905\n",
      "Recall = 0.7352941176470589\n",
      "F1 = 0.03927729772191673\n",
      "TP=75, FP=3642, FN=27\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(np_chunks, true_data_lst)\n",
    "rec, TP, FN = recall(np_chunks, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(np_chunks):\n",
    "    keep, remove = [], []\n",
    "    for i in np_chunks:\n",
    "        if i not in stop and '-' not in i and len(i) > 3:\n",
    "            keep.append(i)\n",
    "        else:\n",
    "            remove.append(i)\n",
    "    return keep, remove\n",
    "\n",
    "data_preprocess_removed, preprocess_removed = preprocess(np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096, 1328)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_preprocess_removed), len(preprocess_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.03482824427480916\n",
      "Recall = 0.7156862745098039\n",
      "F1 = 0.06642402183803459\n",
      "TP=73, FP=2023, FN=29\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(data_preprocess_removed, true_data_lst)\n",
    "rec, TP, FN = recall(data_preprocess_removed, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [i.strip() for i in COMMON_DIR]\n",
    "\n",
    "def remove_google_common(np_chunks):\n",
    "    keep, remove = [], []\n",
    "    for i in np_chunks:\n",
    "        if i not in common:\n",
    "            keep.append(i)\n",
    "        else:\n",
    "            remove.append(i)\n",
    "            \n",
    "    return keep, remove\n",
    "\n",
    "data_common_removed, common_removed = remove_google_common(data_preprocess_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1413, 683)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_common_removed), len(common_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.05024769992922859\n",
      "Recall = 0.696078431372549\n",
      "F1 = 0.09372937293729372\n",
      "TP=71, FP=1342, FN=31\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(data_common_removed, true_data_lst)\n",
    "rec, TP, FN = recall(data_common_removed, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDD = nltk.sent_tokenize(full_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_chunks = set()\n",
    "ner_chunks_cat = set()\n",
    "for sent in DDD:\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        ner_chunks.add(ent.text)\n",
    "        ner_chunks_cat.add(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1261"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_chunks = [i.lower() for i in ner_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ner(np_chunks):\n",
    "    keep, remove = [], []\n",
    "    for i in np_chunks:\n",
    "        if i not in sss:\n",
    "            keep.append(i)\n",
    "        else:\n",
    "            remove.append(i)\n",
    "    return keep, remove\n",
    "\n",
    "data_ner_removed, ner_removed = remove_ner(data_common_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 1268)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_ner_removed), len(ner_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.07586206896551724\n",
      "Recall = 0.10784313725490197\n",
      "F1 = 0.08906882591093118\n",
      "TP=11, FP=134, FN=91\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(data_ner_removed, true_data_lst)\n",
    "rec, TP, FN = recall(data_ner_removed, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = dict(collections.Counter(nltk.word_tokenize(' '.join([lemmatizer.lemmatize(i).strip(punctuation) for i in ' '.join(data).split()]))))\n",
    "# maxx = sorted(wc.items(), key=lambda k: k[1], reverse=True)[0][1]\n",
    "# wc_freq = {k: v/maxx for k, v in wc.items()}\n",
    "\n",
    "# plt.bar(range(len(wc_freq)), list(wc_freq.values()), align='center')\n",
    "# plt.xticks(range(len(wc_freq)), list(wc_freq.keys()))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_high_low_freq(np_chunks):\n",
    "#     keep, remove = [], []\n",
    "#     for i in np_chunks:\n",
    "#         if i in wc_freq:\n",
    "#             if wc_freq[i] < 0.95:\n",
    "#                 keep.append(i)\n",
    "#             else:\n",
    "#                 remove.append(i)\n",
    "#         else:\n",
    "#             remove.append(i)\n",
    "#     return keep, remove\n",
    "\n",
    "\n",
    "# data_freq_removed, freq_removed = remove_high_low_freq(data_preprocess_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data_freq_removed:\n",
    "#     if '-' in i:\n",
    "#         print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_freq_removed), len(freq_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prec, TP, FP = precision(data_freq_removed, true_data_lst)\n",
    "# rec, TP, FN = recall(data_freq_removed, true_data_lst)\n",
    "\n",
    "# print (f\"Precision = {prec}\")\n",
    "# print (f\"Recall = {rec}\")\n",
    "# print (f\"F1 = {f1score(prec, rec)}\")\n",
    "# print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_noisy_chars(np_chunks):\n",
    "    keep, remove = [], []\n",
    "    for i in np_chunks:\n",
    "        if len(re.findall(r'(.)\\1{2,}', i)):\n",
    "            remove.append(i)\n",
    "        else:\n",
    "            keep.append(i)\n",
    "    return keep, remove\n",
    "\n",
    "data_noisy_removed, noisy_removed = remove_repeated_noisy_chars(data_ner_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 7)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_noisy_removed), len(noisy_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haaa', 'hmmm', 'shhhh', 'haaaaaa', 'aaaargh', 'aaaaaaaaaaargh', 'shhh']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.043029259896729774\n",
      "Recall = 0.49019607843137253\n",
      "F1 = 0.07911392405063292\n",
      "TP=50, FP=1112, FN=52\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(data_noisy_removed, true_data_lst)\n",
    "rec, TP, FN = recall(data_noisy_removed, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_corpus(np_chunks):\n",
    "    keep, remove = [], []\n",
    "    for i in np_chunks:\n",
    "        if i in wc:\n",
    "            c1 = wc[i]\n",
    "            if i in brown_words:\n",
    "                c2 = brown_words[i]\n",
    "                if c1 > c2:\n",
    "                    keep.append(i)\n",
    "                else: remove.append(i)\n",
    "            else:\n",
    "                keep.append(i)\n",
    "        else:\n",
    "            keep.append(i)\n",
    "            \n",
    "    return keep, remove\n",
    "\n",
    "data_specific_removed, specific_removed = remove_specific_corpus(data_noisy_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 695)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_specific_removed), len(specific_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.06638115631691649\n",
      "Recall = 0.30392156862745096\n",
      "F1 = 0.10896309314586994\n",
      "TP=31, FP=436, FN=71\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(data_specific_removed, true_data_lst)\n",
    "rec, TP, FN = recall(data_specific_removed, true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "filepath = \"/home/prakhar/Downloads/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n",
    "#extracting words7 vectors from google news vector\n",
    "embeddings_index = {}\n",
    "for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n",
    "    coefs = np.asarray(vector, dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    #feature vector is initialized as an empty array\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in embeddings_index.keys():\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = avg_feature_vector(full_data, model= embeddings_index, num_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "\n",
    "d = pd.DataFrame(data_specific_removed, columns=['glossary'])\n",
    "d['sim'] = d['glossary'].apply(lambda x: distance.cosine(avg_feature_vector(x, model= embeddings_index, num_features=300), context_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 2)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = d.sort_values('sim', ascending=False).head(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.06125\n",
      "Recall = 0.4803921568627451\n",
      "F1 = 0.10864745011086474\n",
      "TP=49, FP=751, FN=53\n"
     ]
    }
   ],
   "source": [
    "prec, TP, FP = precision(dd['glossary'].tolist(), true_data_lst)\n",
    "rec, TP, FN = recall(dd['glossary'].tolist(), true_data_lst)\n",
    "\n",
    "print (f\"Precision = {prec}\")\n",
    "print (f\"Recall = {rec}\")\n",
    "print (f\"F1 = {f1score(prec, rec)}\")\n",
    "print (f\"TP={len(TP)}, FP={len(FP)}, FN={len(FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stra',\n",
       " 'argus',\n",
       " 'infusion',\n",
       " 'mortis',\n",
       " 'multilevel',\n",
       " 'nimbus',\n",
       " 'discus',\n",
       " 'boater',\n",
       " 'ember',\n",
       " 'vein',\n",
       " 'ensnaring',\n",
       " 'heartstrings',\n",
       " 'headmistress',\n",
       " 'patil',\n",
       " 'headmaster',\n",
       " 'hygienic',\n",
       " 'sens',\n",
       " 'stalagmite',\n",
       " 'trevor',\n",
       " 'sleepiness',\n",
       " 'javelin',\n",
       " 'emporium',\n",
       " 'smelting',\n",
       " 'pigtail',\n",
       " 'locomotor',\n",
       " 'newscaster',\n",
       " 'eyeglass',\n",
       " 'coil',\n",
       " 'weatherman',\n",
       " 'parkinson',\n",
       " 'baron',\n",
       " 'fastening',\n",
       " 'stargazer',\n",
       " 'palomino',\n",
       " 'detention',\n",
       " 'wormwood',\n",
       " 'pupil',\n",
       " 'wastepaper',\n",
       " 'cleansweep',\n",
       " 'supple',\n",
       " 'albus',\n",
       " 'ruff',\n",
       " 'tinned',\n",
       " 'passageway',\n",
       " 'phial',\n",
       " 'sheared',\n",
       " 'fungi',\n",
       " 'pellet',\n",
       " 'tendril',\n",
       " 'fang',\n",
       " 'warty',\n",
       " 'squid',\n",
       " 'tabby',\n",
       " 'witchcraft',\n",
       " 'friar',\n",
       " 'unseated',\n",
       " 'goshawk',\n",
       " 'marge',\n",
       " 'dolphin',\n",
       " 'spying',\n",
       " 'beetle',\n",
       " 'aconite',\n",
       " 'elixir',\n",
       " 'transfiguration',\n",
       " 'seared',\n",
       " 'gorgon',\n",
       " 'teabags',\n",
       " 'defrosting',\n",
       " 'snout',\n",
       " 'quaffle',\n",
       " 'bluebell',\n",
       " 'bott',\n",
       " 'bowler',\n",
       " 'wizarding',\n",
       " 'escalator',\n",
       " 'archway',\n",
       " 'warlock',\n",
       " 'fingertip',\n",
       " 'peppermint',\n",
       " 'clinking',\n",
       " 'chessboard',\n",
       " 'midair',\n",
       " 'unsticking',\n",
       " 'scrawl',\n",
       " 'dundee',\n",
       " 'dormitory',\n",
       " 'snakelike',\n",
       " 'alchemist',\n",
       " 'bloodcurdling',\n",
       " 'stalactite',\n",
       " 'spiny',\n",
       " 'drawling',\n",
       " 'goggle',\n",
       " 'refereed',\n",
       " 'muggles',\n",
       " 'bungler',\n",
       " 'prefect',\n",
       " 'sorcerer',\n",
       " 'yvonne',\n",
       " 'thickset',\n",
       " 'stabbing',\n",
       " 'privet',\n",
       " 'bustled',\n",
       " 'disapproving',\n",
       " 'parchment',\n",
       " 'reptile',\n",
       " 'voicing',\n",
       " 'sizing',\n",
       " 'hourglass',\n",
       " 'merlin',\n",
       " 'bodyguard',\n",
       " 'sirius',\n",
       " 'dungeon',\n",
       " 'thinnest',\n",
       " 'frying',\n",
       " 'earmuff',\n",
       " 'staircase',\n",
       " 'festoon',\n",
       " 'crossbow',\n",
       " 'grayish',\n",
       " 'goblet',\n",
       " 'rebellion',\n",
       " 'expelled',\n",
       " 'tartan',\n",
       " 'beak',\n",
       " 'mossy',\n",
       " 'unwrapped',\n",
       " 'waster',\n",
       " 'collapsible',\n",
       " 'furling',\n",
       " 'dudley',\n",
       " 'reckons',\n",
       " 'invisibility',\n",
       " 'whittled',\n",
       " 'monkshood',\n",
       " 'shelling',\n",
       " 'philosopher',\n",
       " 'beastie',\n",
       " 'alibi',\n",
       " 'foghorn',\n",
       " 'bloodiness',\n",
       " 'gamekeeper',\n",
       " 'corridor',\n",
       " 'dimpled',\n",
       " 'duster',\n",
       " 'moleskin',\n",
       " 'bane',\n",
       " 'whisperer',\n",
       " 'sickle',\n",
       " 'ledger',\n",
       " 'werewolf',\n",
       " 'bewitching',\n",
       " 'commentating',\n",
       " 'checkmate',\n",
       " 'trowel',\n",
       " 'tureen',\n",
       " 'bushy',\n",
       " 'hoodlum',\n",
       " 'rowboat',\n",
       " 'icicle',\n",
       " 'peeve',\n",
       " 'flute',\n",
       " 'biased',\n",
       " 'behead',\n",
       " 'dentist',\n",
       " 'gargoyle',\n",
       " 'loosening',\n",
       " 'pinned',\n",
       " 'sacked',\n",
       " 'abou',\n",
       " 'enclose',\n",
       " 'wafting',\n",
       " 'centaur',\n",
       " 'wizardry',\n",
       " 'wheezing',\n",
       " 'turban',\n",
       " 'marmalade',\n",
       " 'cauldron',\n",
       " 'tuft',\n",
       " 'flint',\n",
       " 'thronging',\n",
       " 'referee',\n",
       " 'whelk',\n",
       " 'foretold',\n",
       " 'scarlet',\n",
       " 'teacup',\n",
       " 'yellowish',\n",
       " 'swishy',\n",
       " 'milkman',\n",
       " 'galleon',\n",
       " 'filch',\n",
       " 'savaging',\n",
       " 'alchemy',\n",
       " 'spawn',\n",
       " 'flagged',\n",
       " 'springy',\n",
       " 'lullaby',\n",
       " 'twanging',\n",
       " 'getup',\n",
       " 'treacle',\n",
       " 'meringue',\n",
       " 'rocketed',\n",
       " 'nettle',\n",
       " 'simmering',\n",
       " 'blistering',\n",
       " 'hatch',\n",
       " 'knickerbockers',\n",
       " 'steamrollered',\n",
       " 'cloaked',\n",
       " 'cobbled',\n",
       " 'inky',\n",
       " 'staffroom',\n",
       " 'paisley',\n",
       " 'helmeted',\n",
       " 'bandage',\n",
       " 'heartstring',\n",
       " 'shadowy',\n",
       " 'petunia',\n",
       " 'greener',\n",
       " 'stool',\n",
       " 'glimmered',\n",
       " 'shallow',\n",
       " 'snuffbox',\n",
       " 'singsong',\n",
       " 'licorice',\n",
       " 'woodcraft',\n",
       " 'headless',\n",
       " 'tallest',\n",
       " 'pier',\n",
       " 'clanging',\n",
       " 'silvery',\n",
       " 'hooch',\n",
       " 'speeding',\n",
       " 'paler',\n",
       " 'blundering',\n",
       " 'befuddle',\n",
       " 'whippy',\n",
       " 'tailcoat',\n",
       " 'leaking',\n",
       " 'flick',\n",
       " 'transfigured',\n",
       " 'herbology',\n",
       " 'keeper',\n",
       " 'outstretched',\n",
       " 'cackle',\n",
       " 'gawked',\n",
       " 'craning',\n",
       " 'pelted',\n",
       " 'shrank',\n",
       " 'bendy',\n",
       " 'billowing',\n",
       " 'binoculars',\n",
       " 'scattering',\n",
       " 'refereeing',\n",
       " 'tarantula',\n",
       " 'quidditch',\n",
       " 'towered',\n",
       " 'puncture',\n",
       " 'gloomy',\n",
       " 'chaser',\n",
       " 'tulip',\n",
       " 'amigo',\n",
       " 'whooshing',\n",
       " 'robe',\n",
       " 'spindly',\n",
       " 'clouted',\n",
       " 'wizened',\n",
       " 'trodden',\n",
       " 'sprout',\n",
       " 'whisk',\n",
       " 'teapot',\n",
       " 'carrot',\n",
       " 'unwrapping',\n",
       " 'bathrobe',\n",
       " 'zooming',\n",
       " 'asphodel',\n",
       " 'birdcage',\n",
       " 'facedown',\n",
       " 'flap',\n",
       " 'snitch',\n",
       " 'bezoar',\n",
       " 'scruff',\n",
       " 'stuttering',\n",
       " 'darkly',\n",
       " 'sloped',\n",
       " 'gorilla',\n",
       " 'tosh',\n",
       " 'sniffling',\n",
       " 'bobbing',\n",
       " 'tickling',\n",
       " 'ketchup',\n",
       " 'admirer',\n",
       " 'doormat',\n",
       " 'sorting',\n",
       " 'trapdoor',\n",
       " 'sniffy',\n",
       " 'crate',\n",
       " 'cozy',\n",
       " 'pasty',\n",
       " 'jostled',\n",
       " 'musty',\n",
       " 'starry',\n",
       " 'squeaked',\n",
       " 'caretaker',\n",
       " 'windowsill',\n",
       " 'moldy',\n",
       " 'pumpkin',\n",
       " 'wringing',\n",
       " 'slipper',\n",
       " 'babble',\n",
       " 'changin',\n",
       " 'correcting',\n",
       " 'broomstick',\n",
       " 'rumbling',\n",
       " 'stoat',\n",
       " 'crunching',\n",
       " 'snore',\n",
       " 'smarmy',\n",
       " 'vibrate',\n",
       " 'hooded',\n",
       " 'swooped',\n",
       " 'fume',\n",
       " 'flickering',\n",
       " 'straying',\n",
       " 'muggle',\n",
       " 'kindling',\n",
       " 'steeling',\n",
       " 'draco',\n",
       " 'bossy',\n",
       " 'snowy',\n",
       " 'goblin',\n",
       " 'slouched',\n",
       " 'buttered',\n",
       " 'deafening',\n",
       " 'slimy',\n",
       " 'glittered',\n",
       " 'batty',\n",
       " 'spluttering',\n",
       " 'dreadlock',\n",
       " 'passersby',\n",
       " 'oddball',\n",
       " 'muffin',\n",
       " 'porridge',\n",
       " 'carousel',\n",
       " 'wand',\n",
       " 'potion',\n",
       " 'leaky',\n",
       " 'scribbled',\n",
       " 'duffer',\n",
       " 'wolfsbane',\n",
       " 'pinprick',\n",
       " 'pewter',\n",
       " 'tackled',\n",
       " 'thumpin',\n",
       " 'madam',\n",
       " 'turnip',\n",
       " 'bewitch',\n",
       " 'dumpy',\n",
       " 'bludgers',\n",
       " 'toad',\n",
       " 'crinkled',\n",
       " 'skulking',\n",
       " 'cloak',\n",
       " 'blossoming',\n",
       " 'dribbling',\n",
       " 'crutch',\n",
       " 'nursed',\n",
       " 'quiver',\n",
       " 'gamekeeping',\n",
       " 'snare',\n",
       " 'footstool',\n",
       " 'crumpet',\n",
       " 'swapping',\n",
       " 'overtaking',\n",
       " 'ridgeback',\n",
       " 'piercing',\n",
       " 'toppled',\n",
       " 'stutter',\n",
       " 'freckle',\n",
       " 'flyin',\n",
       " 'perk',\n",
       " 'lumpy',\n",
       " 'woken',\n",
       " 'tottered',\n",
       " 'waddling',\n",
       " 'hushing',\n",
       " 'jell',\n",
       " 'shivered',\n",
       " 'tights',\n",
       " 'fruitcake',\n",
       " 'wight',\n",
       " 'gliding',\n",
       " 'brandished',\n",
       " 'glided',\n",
       " 'drooled',\n",
       " 'fudge',\n",
       " 'onward',\n",
       " 'fluffy',\n",
       " 'stumped',\n",
       " 'twinkled',\n",
       " 'tearful',\n",
       " 'stonewall',\n",
       " 'clamber',\n",
       " 'viridian',\n",
       " 'infernal',\n",
       " 'drooling',\n",
       " 'pudding',\n",
       " 'bewitched',\n",
       " 'fluttered',\n",
       " 'shrunk',\n",
       " 'lopsided',\n",
       " 'fidgeted',\n",
       " 'thundered',\n",
       " 'tyke',\n",
       " 'glittery',\n",
       " 'uric',\n",
       " 'beamed',\n",
       " 'shoo',\n",
       " 'buyin',\n",
       " 'sounding',\n",
       " 'skyward',\n",
       " 'snigger',\n",
       " 'sorrowful',\n",
       " 'catcalling',\n",
       " 'flitted',\n",
       " 'poltergeist',\n",
       " 'codswallop',\n",
       " 'scowl',\n",
       " 'hurtled',\n",
       " 'norris',\n",
       " 'emptying',\n",
       " 'riffraff',\n",
       " 'toothless',\n",
       " 'creepy',\n",
       " 'hovered',\n",
       " 'earshot',\n",
       " 'taped',\n",
       " 'tackling',\n",
       " 'untidy',\n",
       " 'bullied',\n",
       " 'tinge',\n",
       " 'leapt',\n",
       " 'waffling',\n",
       " 'swish',\n",
       " 'transfixed',\n",
       " 'swishing',\n",
       " 'chained',\n",
       " 'mantelpiece',\n",
       " 'gambled',\n",
       " 'slithering',\n",
       " 'swapped',\n",
       " 'pointy',\n",
       " 'pounced',\n",
       " 'squinting',\n",
       " 'snuffling',\n",
       " 'hannah',\n",
       " 'flatten',\n",
       " 'wriggle',\n",
       " 'striding',\n",
       " 'jinxing',\n",
       " 'boastful',\n",
       " 'thrashing',\n",
       " 'takin',\n",
       " 'screech',\n",
       " 'dormouse',\n",
       " 'sneeze',\n",
       " 'phoning',\n",
       " 'puffing',\n",
       " 'sobbed',\n",
       " 'dived',\n",
       " 'droned',\n",
       " 'pawed',\n",
       " 'dunderhead',\n",
       " 'mistaking',\n",
       " 'cornflakes',\n",
       " 'corned',\n",
       " 'scrambling',\n",
       " 'budge',\n",
       " 'scurrying',\n",
       " 'scrabbling',\n",
       " 'snored',\n",
       " 'wriggled',\n",
       " 'disgust',\n",
       " 'queasy',\n",
       " 'tellin',\n",
       " 'nibble',\n",
       " 'neville',\n",
       " 'trotting',\n",
       " 'disgruntled',\n",
       " 'crybaby',\n",
       " 'runnin',\n",
       " 'wrenched',\n",
       " 'tiptoed',\n",
       " 'carefull',\n",
       " 'twitching',\n",
       " 'skinnier',\n",
       " 'cept',\n",
       " 'blinding',\n",
       " 'shoveling',\n",
       " 'swooping',\n",
       " 'whinging',\n",
       " 'tingle',\n",
       " 'flinging',\n",
       " 'makin',\n",
       " 'pebble',\n",
       " 'humbug',\n",
       " 'gritted',\n",
       " 'sprinting',\n",
       " 'galloping',\n",
       " 'sprinted',\n",
       " 'fishy',\n",
       " 'yawning',\n",
       " 'minuscule',\n",
       " 'nosy',\n",
       " 'furious',\n",
       " 'burying',\n",
       " 'punching',\n",
       " 'panted',\n",
       " 'cupboard',\n",
       " 'gawking',\n",
       " 'doorpost',\n",
       " 'ginny',\n",
       " 'hooted',\n",
       " 'loopy',\n",
       " 'ajar',\n",
       " 'troll',\n",
       " 'scribbling',\n",
       " 'eclair',\n",
       " 'guarding',\n",
       " 'blinking',\n",
       " 'sniggered',\n",
       " 'unicorn',\n",
       " 'unraveled',\n",
       " 'spluttered',\n",
       " 'wantin',\n",
       " 'snapped',\n",
       " 'rotted',\n",
       " 'beater',\n",
       " 'dodged',\n",
       " 'squawked',\n",
       " 'hammering',\n",
       " 'tweak',\n",
       " 'chimed',\n",
       " 'ticked',\n",
       " 'shuffled',\n",
       " 'readin',\n",
       " 'twig',\n",
       " 'cryin',\n",
       " 'gibber',\n",
       " 'smarten',\n",
       " 'weirdest',\n",
       " 'sobbing',\n",
       " 'dinky',\n",
       " 'killin',\n",
       " 'unseen',\n",
       " 'ridgebacks',\n",
       " 'slinking',\n",
       " 'squashy',\n",
       " 'wolfing',\n",
       " 'berserk',\n",
       " 'tiptoe',\n",
       " 'darting',\n",
       " 'aunt',\n",
       " 'rubbish',\n",
       " 'grubby',\n",
       " 'gruffly',\n",
       " 'meanin',\n",
       " 'bated',\n",
       " 'twitched',\n",
       " 'nitwit',\n",
       " 'tidier',\n",
       " 'tricky',\n",
       " 'politer',\n",
       " 'wriggling',\n",
       " 'whistle',\n",
       " 'cheered',\n",
       " 'snoozed',\n",
       " 'whooped',\n",
       " 'sideways',\n",
       " 'snot',\n",
       " 'squeak',\n",
       " 'badger',\n",
       " 'lookin',\n",
       " 'broom',\n",
       " 'smirking',\n",
       " 'askew',\n",
       " 'snoozing',\n",
       " 'playin',\n",
       " 'shiny',\n",
       " 'ranting',\n",
       " 'scabby',\n",
       " 'conk',\n",
       " 'shrieked',\n",
       " 'rattled',\n",
       " 'weirdo',\n",
       " 'wrestled',\n",
       " 'quivered',\n",
       " 'turnin',\n",
       " 'howling',\n",
       " 'chickened',\n",
       " 'ouch',\n",
       " 'bludger',\n",
       " 'noticing',\n",
       " 'dumbfounded',\n",
       " 'prickle',\n",
       " 'lotta',\n",
       " 'braver',\n",
       " 'hugged',\n",
       " 'everythin',\n",
       " 'messin',\n",
       " 'havin',\n",
       " 'lurking',\n",
       " 'dawned',\n",
       " 'grumpily',\n",
       " 'mutter',\n",
       " 'maniac',\n",
       " 'booger',\n",
       " 'puddin',\n",
       " 'tidy',\n",
       " 'blankly',\n",
       " 'dreading',\n",
       " 'chased',\n",
       " 'whiskery',\n",
       " 'sneaking',\n",
       " 'ruin',\n",
       " 'watchin',\n",
       " 'knowin',\n",
       " 'clapped',\n",
       " 'barked',\n",
       " 'gasped',\n",
       " 'hullo',\n",
       " 'firsties',\n",
       " 'yawned',\n",
       " 'cheering',\n",
       " 'quailed',\n",
       " 'payin',\n",
       " 'askin',\n",
       " 'swallow',\n",
       " 'chipolata',\n",
       " 'growled',\n",
       " 'hissed',\n",
       " 'mandy',\n",
       " 'knocking',\n",
       " 'sneered',\n",
       " 'gettin',\n",
       " 'hurrying',\n",
       " 'somethin',\n",
       " 'terrified',\n",
       " 'blackpool',\n",
       " 'croaked',\n",
       " 'chappie',\n",
       " 'shinin',\n",
       " 'prickled',\n",
       " 'shoulda',\n",
       " 'imagining',\n",
       " 'moaning',\n",
       " 'summat',\n",
       " 'moaned',\n",
       " 'smatter',\n",
       " 'ahem',\n",
       " 'nothin',\n",
       " 'whimpered',\n",
       " 'sayin',\n",
       " 'scuse',\n",
       " 'forgets',\n",
       " 'speakin',\n",
       " 'flinched',\n",
       " 'chucked',\n",
       " 'ickle',\n",
       " 'chasin',\n",
       " 'dunno',\n",
       " 'groaned',\n",
       " 'blimey',\n",
       " 'sorta',\n",
       " 'whispered',\n",
       " 'shoutin',\n",
       " 'panicking',\n",
       " 'idiot',\n",
       " 'percy',\n",
       " 'wham',\n",
       " 'crikey',\n",
       " 'muttered',\n",
       " 'anythin',\n",
       " 'petrified',\n",
       " 'outta',\n",
       " 'aargh',\n",
       " 'urgh',\n",
       " 'chuckling',\n",
       " 'dratted',\n",
       " 'mighta',\n",
       " 'needin',\n",
       " 'guardin',\n",
       " 'ppose',\n",
       " 'caughty',\n",
       " 'pucey',\n",
       " 'crabbe',\n",
       " 'druidess',\n",
       " 'rubeus',\n",
       " 'emeric',\n",
       " 'ghostie',\n",
       " 'rabbitin',\n",
       " 'promisin',\n",
       " 'adalbert',\n",
       " 'flamel',\n",
       " 'mcguffin',\n",
       " 'chessman',\n",
       " 'mentionin',\n",
       " 'railview',\n",
       " 'longbottom',\n",
       " 'totalusl',\n",
       " 'hedwig',\n",
       " 'mcgonagall',\n",
       " 'alberic',\n",
       " 'higgs',\n",
       " 'vindictus',\n",
       " 'goyle',\n",
       " 'deliverin',\n",
       " 'leviosal',\n",
       " 'weasley',\n",
       " 'hoggy',\n",
       " 'dursley',\n",
       " 'morgana',\n",
       " 'griphook',\n",
       " 'forgettin',\n",
       " 'dedalus',\n",
       " 'grunnings',\n",
       " 'voldemort',\n",
       " 'hengist',\n",
       " 'paracelsus',\n",
       " 'terence',\n",
       " 'diagon',\n",
       " 'hufflepuffs',\n",
       " 'figg',\n",
       " 'ptolemy',\n",
       " 'scamander',\n",
       " 'drooble',\n",
       " 'nosie',\n",
       " 'hufflepuff',\n",
       " 'paddington',\n",
       " 'norbert',\n",
       " 'crockford',\n",
       " 'hermione',\n",
       " 'somefink',\n",
       " 'cliodna',\n",
       " 'petrificus',\n",
       " 'bathilda',\n",
       " 'gallopin',\n",
       " 'turpin',\n",
       " 'ravenclaw',\n",
       " 'granger',\n",
       " 'myst',\n",
       " 'finnigan',\n",
       " 'bletchley',\n",
       " 'millicent',\n",
       " 'nonmagic',\n",
       " 'wizardin',\n",
       " 'slytherin',\n",
       " 'grunnion',\n",
       " 'weasleys',\n",
       " 'erised',\n",
       " 'hogwarts',\n",
       " 'eeylops',\n",
       " 'ehru',\n",
       " 'sniveled',\n",
       " 'smeltings',\n",
       " 'circe',\n",
       " 'binns',\n",
       " 'duddy',\n",
       " 'lecturin',\n",
       " 'ronniekins',\n",
       " 'copyin',\n",
       " 'macdougal',\n",
       " 'ollivander',\n",
       " 'swarmin',\n",
       " 'snape',\n",
       " 'leviosav',\n",
       " 'draconis',\n",
       " 'scabbers',\n",
       " 'dittany',\n",
       " 'broomshed',\n",
       " 'remembrall',\n",
       " 'frightenin',\n",
       " 'oddment',\n",
       " 'undursleyish',\n",
       " 'staggerin',\n",
       " 'enid',\n",
       " 'humberto',\n",
       " 'diggle',\n",
       " 'lamplike',\n",
       " 'popkin',\n",
       " 'slytherins',\n",
       " 'parvati',\n",
       " 'dudleykins',\n",
       " 'algie',\n",
       " 'flump',\n",
       " 'elfric',\n",
       " 'baruffio',\n",
       " 'bulstrode',\n",
       " 'insultin',\n",
       " 'spinnet']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['glossary'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
